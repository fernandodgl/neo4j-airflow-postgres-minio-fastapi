[X] 1) Create Venv:
[x] - Update PIP
[x] - Create Virtual Environment or use WSL
[x] - Activate Virtual Environment or USL
[x] - Create repository on Github

[X] 2) Install packages:
[x] - airflow
[x] - postgres (airflow) 
[x] - spark
[x] - neo4j
[x] - minio
[x] - fastapi

[X] 3) Create folders, export requirements and import data and DAG:
[x] - data
[x] - dags
[x] - Q9Y261.xml
[x] - DAG

[x] 4) Configure tools/services and dependencies:
[x] - airflow 
[x] - spark
[x] - neo4j
[x] - minio
[ ] - services dependencies

[ ] 7)

[ ] 8)

## FILE STRUCTURE
.
├── airflow
│   ├── dags
│   │   ├── parse_uniprot_xml.py
|   |   ├── uniprot_data_pipeline.py
│   ├── Dockerfile
│   ├── entrypoint.sh
│   └── requirements.txt
├── data
├── fastapi
│   │   ├── app.py
|   |   ├── Dockerfile
|   |   ├── requirements.txt
├── logs
├── minio
│   │   ├── Dockerfile
|   |   ├── requirements.txt
|   |   ├── setup.sh
└── docker-compose.yml
└── README.md
└── TODO

###


INCLUDE IN README.md
--------------------
Futures steps
- after downloading dag for parsing, change the folder to /data instead of /dags
- airflow variables for dags (variables are inside the dag)
- there is an issue with a 'fresh start' running the repo. Neo4j container needs to be postponed otherwise you will need to docker-compose 'down' and then 'up'
- implement Neo4j Spark Connector for complex queries (scalability) - Scala or Java needed.
- Implement Spark (PySpark)for parallel processing related to improve scalability in future updates in the project
